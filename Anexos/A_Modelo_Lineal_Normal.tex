\chapter{Análisis bayesiano del modelo lineal normal}\label{Anexo_modelo_lineal}

Para realizar un análisis bayesiano del modelo lineal normal requerimos especificar una distribución inicial para $\theta$ y, mediante el teorema de Bayes, actualizarla para obtener una distribución posterior dados los datos observados. Entonces, primero presento una manipulación de la función de verosimilitud para después ver algunas distribuciones iniciales frecuentemente utilizadas y, finalmente, realizar la actualización de las mismas dados los datos. 

\subsubsection*{Verosimilitud}

Siguiendo a \textcites{GP98}{Congdon06}, manipulemos la función de verosimilitud de la normal multivariada para facilitar la actualización mediante el teorema de Bayes. Observemos que:
\begin{align}\label{eq:modelo_normal_prop}
f(y|\theta) &= \dfrac{1}{\sqrt{(2\pi)|\sigma^2 \mathbb{I}_N|}}exp\left\lbrace -\dfrac{1}{2}(y-X\beta)^T(\sigma^2\mathbb{I}_N)^{-1}(y-X\beta)\right\rbrace \nonumber \\
 &\propto (\sigma^2)^{-n/2}exp\left\lbrace -\dfrac{1}{2\sigma^2}(y-X\beta)^T(y-X\beta)\right\rbrace
\end{align}

En el análisis clásico o frecuentista, el estimador máximo verosímil para los coeficientes $\beta$ es $b=(X^TX)^{-1}X^Ty$. Podemos manipular los términos dentro de la exponencial en la distribución normal con este estimador $b$: 
\begin{align} \label{eq:producto_exponente_normal}
y-X\beta &= y - Xb + Xb - X\beta = (y-Xb) + X(b-\beta) \nonumber \\
\Rightarrow (y-X\beta)^T(y-X\beta) &= \left\lbrace (y-X\beta)^T + \left[X(b-\beta)\right]^T \right\rbrace \Big\{ (y-Xb) + X(b-\beta) \Big\} \nonumber \\
 & = (y-Xb)^T(y-Xb) + (y-Xb)^TX(b-\beta) + \nonumber \\
 &\qquad \left[X(b-\beta)\right]^T(y-Xb) + \left[X(b-\beta)\right]^TX(b-\beta) \nonumber \\
\intertext{y, agrupando los términos cruzados en $k(y,\beta)$,}
\Rightarrow (y-X\beta)^T(y-X\beta) & = (y-Xb)^T(y-Xb) + (b-\beta)^TX^TX(b-\beta) + k(y,\beta)\,.
\end{align}
En realidad, $k(y,\beta) = 0$: 
\begin{align*}
k(y,\beta) &= (y-Xb)^TX(b-\beta) + \left[X(b-\beta)\right]^T(y-Xb)
\intertext{notando que el segundo término es igual al primero pero transpuesto,}
(y-Xb)^TX(b-\beta) &= (y^T - b^TX^T)(Xb-X\beta)
\intertext{sustituyendo el valor de $b$ y considerando que $Xb=y$}
(y-Xb)^TX(b-\beta) &= 
\Big\{ y^T - \left[(X^TX)^{-1}X^Ty \right]^TX^T \Big\}
(y-X\beta) \\
 &= \Big\{y^T - \left[y^TX(X^TX)^{-T}\right]X^T\Big\}
 (y-X\beta)\\
 &= \left[y^T - y^TX(X^{-1}X^{-T})X^T\right]
 (y-X\beta)\\
 &= (y^T - y^T)(y-X\beta)
\intertext{entonces,}
(y-Xb)^TX(b-\beta) &= 0 \quad \Longrightarrow \quad k(y,\beta) = 0\,.
\end{align*}
Podemos entonces sustituir \eqref{eq:producto_exponente_normal} con $k(y,X,\beta) = 0$ en \eqref{eq:modelo_normal_prop}: 
\begin{align*}
f(y|\theta) &\propto (\sigma^2)^{-n/2}exp\left\lbrace -\dfrac{1}{2\sigma^2}\left[(y-Xb)^T(y-Xb) + (b-\beta)^TX^TX(b-\beta)\right] \right\rbrace\\
&\propto (\sigma^2)^{-n/2}exp\left\lbrace -\dfrac{1}{2\sigma^2}\left[(y-Xb)^T(y-Xb) + (\beta-b)^TX^TX(\beta-b)\right] \right\rbrace
\end{align*}
Igual que con el estimador $b$ para los coeficientes, podemos utilizar el estimador máximo verosimil de la varianza, $\hat{\sigma}^2=\dfrac{1}{N}(y-Xb)^T(y-Xb)$, para preparar la verosimilitud de $y|\theta$:
\begin{equation*}
f(y|\theta) \propto (\sigma^2)^{-n/2}exp\left\lbrace -\dfrac{1}{2\sigma^2}\left[N\hat{\sigma}^2 + (\beta-b)^TX^TX(\beta-b)\right] \right\rbrace
\end{equation*}
Notemos ahora que si la varianza $\sigma^2$ fuera conocida podríamos descomponer esta distribución en dos partes, una de las cuales tiene la forma del kernel de una distribución normal para $\beta|\sigma^2$, lo que sugiere ya la familia conjugada de distribuciones iniciales:
\begin{equation*}
f(y|\theta) \propto exp\left\lbrace -\dfrac{1}{2\sigma^2}\left[(\beta-b)^TX^TX(\beta-b)\right] \right\rbrace (\sigma^2)^{-N/2} exp\left\lbrace -\dfrac{N\hat{\sigma}^2}{2\sigma^2}\right\rbrace \,.
\end{equation*}
Finalmente, en este contexto resultará más fácil trabajar en términos de precisiones que de varianzas. Si definimos la precisión de una variable normal como $\tau=\dfrac{1}{\sigma^2}$, tenemos que la función de verosimilitud en el modelo normal se puede representar como sigue: 
\begin{equation} \label{eq:verosimilitud_modelo_normal}
p(y|\theta) \propto exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b)^TX^TX(\beta-b)\right] \right\rbrace \tau^{N/2}exp\left\lbrace -\dfrac{N\hat{\sigma}^2\tau}{2}\right\rbrace \,.
\end{equation}

\subsubsection*{Distribuciones iniciales}

La primera distribución inicial que podríamos plantear sería la distribución conjugada. Recordemos que esta debe tener la misma forma funcional que la verosimilitud, por lo que \eqref{eq:verosimilitud_modelo_normal} sugiere lo siguiente: 
\begin{equation*}
f(\theta)= f(\beta,\tau) \propto exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b_0)^TT_0(\beta-b_0)\right] \right\rbrace \tau^{a/2} exp\left\lbrace -\dfrac{r\tau}{2}\right\rbrace \,,
\end{equation*}
donde $b_0$, $T_0$, $a$ y $r$ sean algunos parámetros convenientes. Con esta forma, podemos determinar la familia conjugada en un proceso de dos pasos. En primer lugar, asumimos que la varianza o precisión está dada, lo que permite definir una distribución inicial para $\beta|\tau$. Posteriormente, determinaremos la distribución inicial conjugada para $\tau$. Es decir, separaremos la distribución inicial en dos: $f(\theta)=f(\beta,\tau)=f(\beta|\tau)f(\tau)$.\\ 

La distribución condicional resulta ser una normal centrada en $b_0$ y con precisión $\tau T_0$, por lo que debemos completarla multiplicando por $1=\tau^{(d-d)/2}$, donde $d$ es el número de coeficientes, incluyendo a $\beta_0$. Así: 
\begin{align} \label{eq:distr_ng}
f(\theta)= f(\beta|\tau)f(\tau) &\propto exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b_0)^TT_0(\beta-b_0)\right] \right\rbrace \tau^{a/2} exp\left\lbrace -\dfrac{r\tau}{2}\right\rbrace \nonumber \\
&\propto \tau^{(d - d)/2} exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b_0)^TT_0(\beta-b_0)\right] \right\rbrace \tau^{a/2} exp\left\lbrace -\dfrac{r\tau}{2}\right\rbrace \,. \nonumber\\
\intertext{Con lo que} \nonumber 
f(\beta|\tau) &\propto \tau^{d/2} exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b_0)^TT_0(\beta-b_0)\right] \right\rbrace \,\text{y} \nonumber \\
f(\tau) &\propto \tau^{(a-d)/2} exp\left\lbrace -\dfrac{r\tau}{2}\right\rbrace \,.
\end{align}

La distribución inicial de $\tau$ también ya tiene una forma conocida: es proporcional a una gamma. Para verlo solo basta con un poco de álgebra para verificar que el parámetro de forma debe ser $a_0 = (a-d+2)/2=(a^\star-d)/2$ con $a^\star=a+2$ y el de tasa $r_0 = r/2$. Por lo tanto, en su conjunto, tenemos que $\theta$ tiene una distribución inicial \textit{Normal-Gamma}: 
\begin{align} 
\theta &= (\beta,\tau) \sim NG_d\left(b_0,T_0,a_0=\dfrac{a^\star-d}{2},r_0 =\dfrac{r}{2}\right) \nonumber \\
\intertext{de forma que}
\beta|\tau &\sim N_d(b_0,\tau T_0) \;\text{y}\; \tau \sim \Gamma\left(a_0 = \dfrac{a^\star-d}{2}, r_0 = \dfrac{r}{2}\right) \,.
\label{eq:normal_gamma}
\end{align}
Cabe hacer notar que esta distribución inicial conjugada es propia siempre que $a^\star > d $, $r > 0$ y $B_0 = T_0^{-1}$ sea positiva definida.\\ 

Por otro lado, si se buscan distribuciones iniciales más vagas, resulta que también es posible obtener distribuciones mínimo informativas límites de esta conjugada. Por ejemplo, aunque es impropia, la inicial de Jeffreys es de esa forma con los siguientes límites: $a^\star \rightarrow d $, $r \rightarrow 0$ y $B_0 = T_0^{-1} \rightarrow \mathbf{O}$. La \eqref{eq:distr_ng} se reduce a la siguiente expresión \parencite{GP98}: 
\begin{equation} \label{eq:jeffreys_ng}
f(\theta) = f(\beta,\tau) \propto \tau^{(d-2)/2}
\end{equation}

\subsubsection*{Distribuciones finales}

Consideremos para la actualización el caso general de la distribución inicial normal gamma de \eqref{eq:normal_gamma}. 
\begin{align} \label{eq:modelo_normal_pre_bayes}
y|\theta &\sim N_N(X\beta,\sigma^2\mathbb{I}_N) \quad \text{tal que} \quad \theta = (\beta, \sigma^2) \sim f(\beta,\sigma^2) \nonumber \\
\beta|\tau &\sim N_d(b_0,\tau T_0) \nonumber \\ 
\tau &\sim \Gamma\left(a_0 = \dfrac{a^\star-d}{2},r_0 = \dfrac{r}{2}\right) \,.
\end{align}

Aplicaremos el teorema de Bayes con base en \eqref{eq:verosimilitud_modelo_normal} y \eqref{eq:distr_ng} buscando, al tener una inicial conjugada, mantener la forma de normal gamma. Esto es, la verosimilitud la podemos ver también como el producto de dos distribuciones, una normal para $\beta|\tau$ centrada en el estimador máximo verosímil $b$ y con precisión $\tau X^TX$ y una gamma para $\tau$ utilizando el estimador máximo verosímil de la varianza $\hat{\sigma}^2$. 
\begin{align} \label{eq:post_normal_gamma_todo}
f(\theta|y) &\propto f(y|\theta)f(\theta)\nonumber \\
&\propto exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b)^TX^TX(\beta-b)\right] \right\rbrace \tau^{N/2} exp\left\lbrace -\dfrac{N\hat{\sigma}^2\tau}{2}\right\rbrace \nonumber \\
&\qquad  \tau^{d/2} exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b_0)^TT_0(\beta-b_0)\right] \right\rbrace \tau^{(a-d)/2} exp\left\lbrace -\dfrac{r\tau}{2}\right\rbrace \nonumber \\
&\propto \tau^{d/2} exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b)^TX^TX(\beta-b) + (\beta-b_0)^TT_0(\beta-b_0)\right]\right\rbrace \nonumber \\
& \qquad \tau^{(N - d + a)/2} exp\left\lbrace -\dfrac{N\hat{\sigma}^2 + r}{2}\tau\right\rbrace \,.
\end{align}
Ahora simplifiquemos el término dentro de la primera exponencial para que coincida con el kernel de una distribución normal. 
\begin{align} \label{eq:termino_exp_post_normal_gamma}
&(\beta - b)^T X^TX (\beta-b) + (\beta - b_0)^T T_0 (\beta-b_0) \nonumber \\
&\; = \beta^TX^TX\beta - \beta^TX^TXb - b^TX^TX\beta + b^TX^TXb \,+ \nonumber \\ 
&\qquad \beta^TT_0\beta - \beta^TT_0b_0 - b_0^TT_0\beta + b_0^TT_0b_0  \nonumber \\
\intertext{notando que todos estos términos son escalares de forma que sus transpuestos son ellos mismos, así como que $T_0^T=T_0$,}\nonumber 
&\; = \beta^TX^TX\beta - 2\beta^TX^TXb + b^TX^TXb + \beta^TT_0\beta - 2\beta^TT_0b_0 + b_0^TT_0b_0 \nonumber \\
&\; = \beta^T(X^TX + T_0)\beta - 2\beta^TX^TXb - 2\beta^TT_0b_0 + b^TX^TXb + b_0^TT_0b_0 \nonumber \\
\intertext{definiendo $T_1=X^TX + T_0 \quad$ y $\quad g(X,y)=b^TX^TXb + b_0^TT_0b_0$,} \nonumber 
&\; = \beta^TT_1\beta - 2\beta^TX^TXb - 2\beta^TT_0b_0 + g(X,y) \nonumber \\
&\; = \beta^TT_1\beta - 2\beta^T\left[X^TXb + T_0b_0\right] + g(X,y) \nonumber \\
\intertext{definiendo $b_1=T_1^{-1}(X^TXb+T_0b_0)$ y completando el cuadrado:} \nonumber
&\; = \beta^TT_1\beta - 2\beta^TT_1b_1 + g(X,y) \nonumber \\
&\; = (\beta-b1)^TT_1(\beta-b_1) + g(X,y) - b_1^TT_1b_1 \,.
\end{align}
Con esta manipulación de términos, ya podemos tener la distribución posterior de $\beta|\tau$, sustituyendo \eqref{eq:termino_exp_post_normal_gamma} en \eqref{eq:post_normal_gamma_todo}, como una normal $d$-variada con media $b_1$ y precisión $\tau T_1$: 
\begin{align*}
f(\theta|y) &\propto \tau^{d/2} exp\left\lbrace -\dfrac{\tau}{2}\left[(\beta-b1)^TT_1(\beta-b_1)\right]\right\rbrace \\
& \qquad \tau^{(N - d + a)/2} exp\left\lbrace -\dfrac{N\hat{\sigma}^2 + g(X,y) - b_1^TT_1b_1 + r}{2}\tau\right\rbrace \,.
\end{align*}
La nueva media $b_1=T_1^{-1}(X^TXb+T_0b_0)$ puede verse como un promedio de las medias originales--- la de la inicial y el estimador máximo verosímil--- ponderadas por sus precisiones \parencite{Congdon06}. La nueva precisión es simplemente la suma de las precisiones originales.\\

Ahora debemos encontrar los nuevos parámetros de forma y tasa para la distribución posterior de $\tau$. Igualando el exponente de $\tau$ en la última expresión a $a_1-1$, donde $a_1$ es el nuevo parámetro de forma, para satisfacer la representación de una distribución gamma se llega a que $a_1=(N-d+a^\star)/2$. El nuevo parámetro de tasa $r_1$ requiere ser un poco más explícitos: 
\begin{align*}
r_1 &= \dfrac{N\hat{\sigma}^2 + g(X,y) - b_1^TT_1b_1 + r}{2} \\
&= \dfrac{(y-Xb)^T(y-Xb) + b^TX^TXb + b_0^TT_0b_0 - b_1^TT_1b_1 + r}{2} \,.
\end{align*}
Pero resulta que $(y-Xb)^T(y-Xb) + b^TX^TXb = y^Ty$: 
\begin{align} \label{eq:aux_para_jeffreys_ng}
(y-Xb)^T(y-Xb) + b^TX^TXb &= y^Ty - 2y^TXb + b^TX^TXb + b^TX^TXb \nonumber \\
&= y^Ty - 2y^TXb + 2b^TX^TXb \nonumber \\
&= y^Ty - 2b^TX^TXb + 2b^TX^TXb \nonumber \\
&= y^Ty \,.
\end{align}
Por lo que, en realidad, 
\begin{equation*}
r_1 = \dfrac{y^Ty + b_0^TT_0b_0 - b_1^TT_1b_1 + r}{2} \,.
\end{equation*}
Con esto tenemos que la actualización de las \eqref{eq:modelo_normal_pre_bayes} nos llevan al siguiente modelo conjugado: 
\begin{align} \label{eq:modelo_normal_post_bayes}
y|\theta &\sim N_N(X\beta,\sigma^2\mathbb{I}_N) \quad \text{tal que} \quad \theta = (\beta, \sigma^2) \sim f(\beta,\sigma^2) \nonumber \\
\beta|\tau &\sim N_d(b_0,\tau T_0) \qquad  \tau \sim \Gamma\left(a_0 = \dfrac{a^\star-p}{2},r_0 = \dfrac{r}{2}\right) \nonumber \\ 
\beta|\tau , y &\sim N_p(b_1,\tau T_1) \qquad \tau|y \sim \Gamma\left(a_1,r_1\right) \nonumber \\
\intertext{con $a^\star > d $, $r > 0$ y $B_0 = T_0^{-1}$ positiva definida y tal que} \nonumber
T_1 &= X^TX+T_0 \qquad b_1 = T_1^{-1}(X^TXb+T_0b_0) = T_1^{-1}(X^Ty+T_0b_0),\nonumber \\ 
a_1 &= \dfrac{N-d+a^\star}{2} \qquad r_1 = \dfrac{y^Ty + b_0^TT_0b_0 - b_1^TT_1b_1 + r}{2}
\end{align}
donde $b=(X^TX)^{-1}X^Ty$ es el estimador máximo verosímil de $\beta$. Más aún, si en lugar de utilizar como distribución inicial una normal gamma de esta forma se utiliza la inicial de Jeffreys de \eqref{eq:jeffreys_ng}, podemos utilizar estas expresiones para hacer la actualización--- aprovechando el carácter que la inicial de Jeffreys tiene como límite de conjugadas--- considerando $a^\star \rightarrow d $, $r \rightarrow 0$ y $B_0 = T_0^{-1} \rightarrow \mathbf{O}$, por lo que se tendrían: 
\begin{equation*}
T_1 = X^TX \qquad b1 = b \qquad a_1 = \dfrac{N}{2} \qquad r_1 = \dfrac{y^Ty - b^TX^TXb}{2}=\dfrac{N\hat{\sigma}^2}{2} \,
\end{equation*}
donde la equivalencia del estimador máximo verosímil $\hat{\sigma}^2$ puede verificarse con \eqref{eq:aux_para_jeffreys_ng}.\\

